# -*- coding: utf-8 -*-
"""gan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pc5aDw_-u9jw5Dtv-mFKf2H4Tpz4Co8M
"""
from __future__ import print_function, division
import itertools
import logging
import tensorflow as tf
import keras.backend as K
import matplotlib.pyplot as plt
from matplotlib.offsetbox import AnchoredText
import numpy as np
import pandas as pd
import six
from keras import optimizers, regularizers
from keras.callbacks import *
from keras.layers import (LSTM, Activation, Add, Conv1D, Conv2D, Dense,
                          Dropout, Embedding, Flatten, GlobalMaxPooling1D,
                          GlobalMaxPooling2D, Input, MaxPooling1D,
                          MaxPooling2D, Reshape, SimpleRNN, UpSampling1D)
from tensorflow.keras.layers import BatchNormalization
from keras.models import Model, Sequential, load_model
from tensorflow.keras.utils import plot_model
from sklearn import preprocessing
from sklearn.metrics import confusion_matrix
from tensorflow.python import debug as tf_debug
from sklearn.metrics import classification_report
import time
import datetime
from sklearn import feature_extraction
from sklearn.compose import ColumnTransformer
from keras import regularizers
from sklearn.utils.multiclass import unique_labels


from keras.datasets import mnist
from keras.layers import Input, Dense, Reshape, Flatten, Dropout
from keras.layers import BatchNormalization, Activation, ZeroPadding2D
from tensorflow.keras.layers import LeakyReLU
from keras.layers.convolutional import UpSampling2D, Conv2D
from keras.models import Sequential, Model
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import sys
import numpy as np
import os
import math

"""GAN"""

def GAN_oneHotLabel(train):

    return pd.get_dummies(train)

def GAN_buildTrain(train, labelEncoder):
    
    typeLabel = list()
    featuresName = list(train.columns[:-(len(labelEncoder.classes_))])
    [typeLabel.append('type_' + str(t)) for t in labelEncoder.classes_]
    X_train = np.array(train[:train.shape[0]][featuresName])
    Y_train = np.array(train[:train.shape[0]][typeLabel])
    
    return X_train, Y_train

"""GAN Model"""

def build_generator():

    model = Sequential()

    model.add(Dense(16, input_dim=latent_dim))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Dense(16))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Dense(6))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))
    model.summary()

    noise = Input(shape=(latent_dim,))
    img = model(noise)

    return Model(noise, img)

def build_discriminator():

    model = Sequential()

    model.add(Dense(16, input_shape=(int(6),)))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dense(16))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dense(1, activation='sigmoid'))
    model.summary()

    img = Input(shape=(int(6),))
    validity = model(img)

    return Model(img, validity)

def GAN_train(epochs, X_train, Y_train, batch_size, GAN_train_count, Label):

    count = 0
    # Adversarial ground truths
    valid = np.ones((batch_size, 1))
    print(valid.shape)
    fake = np.zeros((batch_size, 1))
    print(fake.shape)
    GAN_train_history = pd.DataFrame()

    

    for epoch in range(epochs):

        # ---------------------
        # Train Discriminator
        # ---------------------

        # Select a random batch of images
        idx = np.random.randint(0, X_train.shape[0], batch_size)
        real_train = X_train[idx]
        
        idx = np.random.randint(0, Y_train.shape[0], batch_size)
        val_train = Y_train[idx]

        noise = np.random.normal(0, 1, (batch_size, latent_dim))

        # Generate a batch of new images
        gen_imgs = generator.predict(noise)

        print(real_train.shape)
        print(val_train.shape)

        # Train the discriminator
        d_loss_real = discriminator.train_on_batch(real_train, val_train)
        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # ---------------------
        #  Train Generator
        # ---------------------

        noise = np.random.normal(0, 1, (batch_size, latent_dim))
        print(noise.shape)

        # Train the generator (to have the discriminator label samples as valid)
        g_loss = combined.train_on_batch(noise, val_train)



        print("%d [D loss: %f, acc.: %.2f%%] [G loss: %f]" % (epoch, d_loss[0], 100*d_loss[1], g_loss))
       
        a = np.concatenate((Label,  d_loss[0], np.round(100*d_loss[1], 2), g_loss), axis=None)
        # print(a.shape)
        a = np.reshape(a ,(1,4))
        s = pd.DataFrame(a, columns=['Label','D_Loss', 'Acc', 'G_Loss'])
        
        if GAN_train_count == 0:
            GAN_train_history = GAN_train_history.append(s, ignore_index=True)
            GAN_train_history.to_csv('GAN_Training_History_' + Label + '.csv', index=False)
            GAN_train_count = GAN_train_count + 1
        else:
            GAN_train_history = pd.read_csv('GAN_Training_History_' + Label + '.csv')
            GAN_train_history = GAN_train_history.append(s, ignore_index=True)
            GAN_train_history.to_csv('GAN_Training_History_' + Label + '.csv', index=False)

        if (abs(g_loss) - abs(d_loss[0])) <= 0.05:
            count = count + 1
            print(count)
        if count == 500:
            generator.save(str(Label) + '_generator.h5')
            print('Final Count: ', count)
            print('D_loss: ', d_loss[0])
            print('G_loss: ', g_loss)
            time.sleep(3)
            break
            

def sample_images(epoch):
    # r, c = 1, 1
    noise = np.random.normal(0, 1, (1, latent_dim))
    # print('noise: ', noise)
    gan_packet = generator.predict(noise)

    # print('gan_packet: ',gan_packet)
    # print('gan_packet.shape: ', gan_packet.shape)
    
    return abs(gan_packet)

"""combine with classify"""

def classi_readTrain():

    trainFilePath = 'Datasets/141_train_1_(new).csv'
    # trainFilePath = 'Cut_141_7_test_withOtherFeature_V2(seperate)_withoutHeader.csv'
    # trainFilePath = '141_train_FilterIP_3_seperate(withoutHeader)_plus.csv'
    # trainFilePath = 'train.csv'
    columnName = [str(i) for i in range(10)] + ['type']
    train = pd.read_csv(trainFilePath, names=columnName)
    # train.drop(['0'], axis=1, inplace=True)

    return train

def classi_readTesting():

    # trainFilePath = '141_7_test_withOtherFeature_V2(seperate)_withoutHeader.csv'
    trainFilePath = 'Datasets/Cut_141_7_test_withOtherFeature_V2(seperate)_withoutHeader.csv'
    # trainFilePath = 'test.csv'
    columnName = [str(i) for i in range(int(10))] + ['type']
    test = pd.read_csv(trainFilePath, names=columnName)
    print(test.head())
    
    # test.drop(['0'], axis=1, inplace=True)

    return test

def change_label_to_number(test):
    for i in range(len(test)): #將type換成number代替
        print('--------before change-----------')
        print(i, ' : ', test['type'][i])
        if test['type'][i] == 'AppleiCloud':
            test['type'][i] = 0
        elif test['type'][i] == 'AppleiTunes':
            test['type'][i] = 1
        elif test['type'][i] == 'Dropbox':
            test['type'][i] = 2
        elif test['type'][i] == 'FTP_DATA':
            test['type'][i] = 3
        elif test['type'][i] == 'Facebook':
            test['type'][i] = 4
        elif test['type'][i] == 'GMail':
            test['type'][i] = 5
        elif test['type'][i] == 'Github':
            test['type'][i] = 6
        elif test['type'][i] == 'GoogleDrive':
            test['type'][i] = 7
        elif test['type'][i] == 'GoogleHangoutDuo':
            test['type'][i] = 8
        elif test['type'][i] == 'GoogleServices':
            test['type'][i] = 9
        elif test['type'][i] == 'Instagram':
            test['type'][i] = 10
        elif test['type'][i] == 'MS_OneDrive':
            test['type'][i] = 11
        elif test['type'][i] == 'NetFlix':
            test['type'][i] = 12
        elif test['type'][i] == 'Skype':
            test['type'][i] = 13
        elif test['type'][i] == 'Snapchat':
            test['type'][i] = 14
        elif test['type'][i] == 'SoundCloud':
            test['type'][i] = 15
        elif test['type'][i] == 'Spotify':
            test['type'][i] = 16
        elif test['type'][i] == 'Steam':
            test['type'][i] = 17
        elif test['type'][i] == 'TeamViewer':
            test['type'][i] = 18
        elif test['type'][i] == 'Telegram':
            test['type'][i] = 19
        elif test['type'][i] == 'Twitter':
            test['type'][i] = 20
        elif test['type'][i] == 'WhatsApp':
            test['type'][i] = 21
        elif test['type'][i] == 'Wikipedia':
            test['type'][i] = 22
        elif test['type'][i] == 'YouTube':
            test['type'][i] = 23
        elif test['type'][i] == 'eBay':
            test['type'][i] = 24
        print('--------after change-----------')
        print(i, ' : ', test['type'][i])
    return test

def get_dict(FL_epoch, rounds, fulltrain): #combine_test
    Not_Achieve_Label = list()
    print('@@@@@@@@@ Read Testing Report @@@@@@@@@@@')
    ##### FL_次數_GAN_次數_GAN_report
    report = pd.read_csv('FL_' + str(FL_epoch) + '_GAN_' + str(rounds) + '_GAN_report.csv', index_col=0)
    print(report)
    print('Labels : ', len(report))
    print('------------- Finished ----------------')
    print('@@@@@@@@@@ Transpose @@@@@@@@@@@@@')
    report = pd.DataFrame(report).transpose()
    report = report.to_dict()
    for total_key, value in report.items():
        Not_Achieve_Label.append(total_key)
    # print(report)
    print(Not_Achieve_Label)
    values_list = list()
    for i in range(len(Not_Achieve_Label)):
        if Not_Achieve_Label[i] == 'FTP_DATA':
            filter = (fulltrain['type'] == 'FTP_DATA')
            data_number = fulltrain[filter]
            values_list.append(math.ceil(len(data_number) * 0.5))
        elif Not_Achieve_Label[i] == 'Snapchat':
            filter = (fulltrain['type'] == 'Snapchat')
            data_number = fulltrain[filter]
            values_list.append(math.ceil(len(data_number) * 0.6))
        elif Not_Achieve_Label[i] == 'SoundCloud':
            filter = (fulltrain['type'] == 'SoundCloud')
            data_number = fulltrain[filter]
            values_list.append(math.ceil(len(data_number) * 1.3))
        elif Not_Achieve_Label[i] == 'eBay':
            filter = (fulltrain['type'] == 'eBay')
            data_number = fulltrain[filter]
            values_list.append(math.ceil(len(data_number) * 1.8))
        else:
            filter = (fulltrain['type'] == Not_Achieve_Label[i])
            data_number = fulltrain[filter]
            if rounds == 0:
                values_list.append(math.ceil(len(data_number) * (0.1 * 1)))
            else:
                # values_list.append(math.ceil(len(data_number) * (0.1 * rounds)))
                values_list.append(math.ceil(len(data_number) * (0.1 * 1)))

    not_achieve_dict = dict(zip(Not_Achieve_Label, values_list))
    print(not_achieve_dict)
    time.sleep(3)
    return not_achieve_dict


"""combine with Trianing set"""


### Control GPU used memory
gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)
sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))

fulltrain = pd.read_csv('fulltrain.csv')
fulltest = pd.read_csv('fulltest.csv')

print('-----------------------Above is to get hashsize and Normalized-----------------------')

temp_count = 0

print('@@@@@@@@@ Quantity of All Sample @@@@@@@@@@@@@@@@@')#important
round = 0
FL_epoch = 0

print('@@@ FL_count @@@@@@@@')
if os.path.exists('FL_count.txt'):
    f = open('FL_count.txt')
    FL_epoch = f.read()
    FL_epoch = int(FL_epoch)
    print(FL_epoch)
    f.close

if os.path.exists('GAN_count.txt'):
    f = open('GAN_count.txt')
    round = f.read()
    round = int(round)
    print(round)
    f.close

not_achieve_dict = get_dict(FL_epoch, round, fulltrain)
print(not_achieve_dict)
############################################ˇ
####資料量 應該搞定
###存生成模型 搞定
print('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@')

if not not_achieve_dict:
    print('All Labels are finished!')
else: 
    for key, value in not_achieve_dict.items():

        train_Static = fulltrain[['3067','3068','3069', '3070','3071','3072','type']]
        # print(train)

        filter = (train_Static['type'] == key) #Get Indicate Data
        oneTrain = train_Static[filter]

        oneHotTrain = GAN_oneHotLabel(oneTrain)

        labelEncoder = preprocessing.LabelEncoder()
        labelEncoder.fit(oneTrain.type.unique())

        X_train, Y_train = GAN_buildTrain(oneHotTrain, labelEncoder)
        # print(X_train)
        # print(X_train.shape)
        # print(Y_train)
        # print(Y_train.shape)

        print("-------------------------------GAN--------------------------------")

        latent_dim = 100

        optimizer = Adam(0.0002, 0.5)

        discriminator = build_discriminator()
        generator     = build_generator()
        ########
        """discriminator"""
        ########
        discriminator      = Model(inputs=discriminator.inputs, outputs=discriminator.outputs)
        discriminator.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])

        print('-------------------------------------------')
        print(discriminator.layers)
        print('-------------------------------------------')

        ########
        """GAN"""
        ########
        generator          = Model(inputs=generator.inputs,outputs=generator.outputs)
        frozen_D           = Model(inputs=discriminator.inputs, outputs=discriminator.outputs)
        frozen_D.trainable = False


        discriminator_trainable_weights = len (discriminator.trainable_weights)  # for asserts, below
        generator_trainable_weights = len (generator.trainable_weights)

        noise_input = Input (shape=(latent_dim,))
        image = generator (noise_input)
        validity_output = frozen_D (image)

        combined = Model (inputs = noise_input, outputs = validity_output, name = 'combined')
        combined.compile (loss = 'binary_crossentropy', optimizer = optimizer)

        assert (len (discriminator.trainable_weights) == discriminator_trainable_weights)#!!
        assert (len (combined.trainable_weights) == generator_trainable_weights)#!!
        """GAN Model END"""

        if os.path.exists(str(key) + '_generator.h5'):
            generator = load_model(str(key) + '_generator.h5')
        else:
            print('GAN for ', key)
            GAN_train(epochs=5000, X_train=X_train, Y_train=Y_train, batch_size=8, GAN_train_count=0, Label=key)

        print('-----------------------------------------------------------------------------------------')

        require_Sample = value #要求生成比數
        generate_Sample = 200 #每次生成一千筆去辨識
        classify_Correct = 0 #該類別被判斷正確的比數
        remain_data = 0
        temp_of_previous_classify_Correct = 0

        while classify_Correct != require_Sample: #判斷該類別正確的總比數要大於該類別要求的生成比數

            gan_sample = []
            label = []

            for i in range(generate_Sample):
                print(i, ':')
                sample = sample_images(i)
                print(type(sample))
                print(sample)
                gan_sample.append(sample)


            np_sample = np.array(gan_sample)
            np_sample = np_sample.reshape([generate_Sample ,6])
            g_sample = pd.DataFrame(np_sample, columns=['3067','3068','3069', '3070','3071','3072']) #Generate Data

            print('-------------------------Final------------------------------')
            print(g_sample)

            train_IP =  fulltest.drop(['3067','3068','3069', '3070','3071','3072'], axis=1) ###

            print('---------------train_IP--------------------')
            print(train_IP)
            print(train_IP.shape)
            print(type(fulltrain))

            print('---------------Indicate Type-------------')
            filter = (train_IP['type'] == key) #取出我們要的label
            print(filter)
            Indicate_Type = train_IP[filter]
            print(Indicate_Type)  #取預處裡的IP

        #     print('----------------How Many Raw of the Type-----------------')
        #     print(Indicate_Type.shape[0])

            while Indicate_Type.shape[0] < generate_Sample:
                Indicate_Type = Indicate_Type.append(Indicate_Type, ignore_index=True) # 因為有些資料及本身不構，因此在取出IP的數量會不夠，因此需要將籍複製到超過要生成的數量
                print('----------------After Plus The Type---------------------------')
                print(Indicate_Type.shape[0])
            

            Random_Indicate_Type = Indicate_Type.sample(n = generate_Sample,random_state=None) #隨機抽幾個出來

            Random_Indicate_Type.reset_index(inplace=True)

            Concat_Indicate_Type = pd.concat([Random_Indicate_Type, g_sample], axis=1) #並與生成數值concat再一起
            Concat_Indicate_Type = Concat_Indicate_Type.drop(['index'],axis=1) #將index刪除

            print('----------------------IP Concat with Other 6 feature------------------')
            print(Concat_Indicate_Type)

            cols = Concat_Indicate_Type.columns.tolist() #將type移至最後面
            cols.insert(3073, cols.pop(cols.index('type')))   
            Concat_Indicate_Type = Concat_Indicate_Type[cols]

            print('------------------After Change Position--------------------')
            print(Concat_Indicate_Type)
            print(type(Concat_Indicate_Type))

            print('-----------------------Testing-----------------------------')
            test = Concat_Indicate_Type.copy() #需要用copy才不會test有什麼改變，Concat_Indicate的值跟著一起變動

            print('------------------Change Type to Number-------------------')
            print('')
            test = change_label_to_number(test)

            print('--------------------Label-------------------------')
            label = test['type'] #將其中的number label取出
            label = np.array(label) #由於預測的classes_是numpy，所以在這也要轉一下
            print(type(label))
            print(label)

            print('--------------------After LoadData----------------------')
            test = test.drop(['type'], axis=1) #放入模型測試不會有type跟著，所以要先Drop調
            print(test)

            print('-------------------Predicting-------------------------')
            print('classify_Correct: ')
            print(classify_Correct)

            # modelFileName = 'g_4_MLPModel' #讀進模型
            # modelFileName = 'w_3_MLPModel' #讀進模型
            modelFileName = 'MLPModel' #讀進模型 #temp_data #important


            #### FL_次數_GAN_次數_modelFileName
            # model_load = load_model(str(round) + '_' + modelFileName + '.h5')
            if round == 0:
                model_load = load_model('FL_' + str(FL_epoch) + '_' + modelFileName + '.h5')
            else:
                model_load = load_model('FL_' + str(FL_epoch) + '_GAN_' + str(round) + '_' + modelFileName + '.h5')


            predict = model_load.predict(test)

            print('-----------------predict shape-----------------------')
            print(predict.shape)

            print('----------------predict class----------------------')
            classes_ = predict.argmax(axis=-1) #取得預測的結果(預測的label)
            print(classes_)
            print(type(classes_))

            print('----------------------predict label and Trye labek shape------------------------')
            print(label)
            print(label.shape)
            print(classes_)
            print(classes_.shape)

            temp_of_previous_classify_Correct = classify_Correct #記住前一次的正確的數量

            data_index = list()

            for i in range(len(classes_)): #將預測的和正確的做比較，並記數每個label預測的正確數
                if classes_[i] == label[i]:
                    
                    if classify_Correct != require_Sample:
                        classify_Correct = classify_Correct + 1

                        data_index.append(i) #將每個預測正確的資料的index存下

            remain_data = classify_Correct - temp_of_previous_classify_Correct #這次新增的判斷正確的數量 - 上次判斷正確數量 = 增加多少筆
        #     #因為classify_Correct不會超過require_Sample，因此可以利用每次的判斷增加的數量去準確的加入到指定的生成數量
        #     #而不會超過指定的數量

            print('')
            if not data_index:
                print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')
                print('No effcetive samples!')
                print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')
                continue

            print('')
            print('----------------------Concat Data------------------')
            print(Concat_Indicate_Type)

            print('---------------------Indicate Data Index--------------')
            print(data_index)

            temp_data = pd.DataFrame()

            for i in data_index: #將記錄下的預測正確的資料存在temp_data裡
                # print(test.iloc[i])
                temp_data = temp_data.append(Concat_Indicate_Type.iloc[[i]], ignore_index=True) #利用iloc抓出index相對應的資料
                #利用iloc必須要是兩層[]，這樣才是DataFrame

            
            print('---------------------Indicate Data------------------')
            print(temp_data)
            print(temp_data.shape)

            if temp_count == 0: #當前類別是否已經存下生成資料了 #important

                #### FL_次數_GAN_次數_temp_data_test
                temp_data.to_csv('FL_'+ str(FL_epoch) + '_GAN_' + str(round) + '_temp_data_test.csv', index=False) #將temp_data存成csv加入倒訓練集中一起訓練
            else:
                read_all_temp_data = pd.read_csv('FL_'+ str(FL_epoch) + '_GAN_' + str(round) + '_temp_data_test.csv')
                temp_data = temp_data.append(read_all_temp_data, ignore_index=True)#與之前所存下的生成資料放在一起
                temp_data.to_csv('FL_'+ str(FL_epoch) + '_GAN_' + str(round) + '_temp_data_test.csv', index=False)#再次存起來

            temp_count = temp_count + 1
            

        print('---------------------------------------------------------------------------------------------------')

#根據report判斷哪個類別沒達到指定效能 ok
#將其存成csv ok
#進入GAN的環節 ok
#各個類別作訓練，並存取其GAN的G模型
#minority的部分就是根據之前所找出的最佳生成數，其餘就是他的資料量的%數(根據每一round不停提升)
#生成結束，加入訓練集訓練 ok
#根據report判斷在csv裡哪個類別有提升，以及是否達標 ok
    #未達標 -> 再去做生成並以下一輪的模型篩選再去訓練 (可能會有第二輪效能是降低的，但仍以第二輪的模型篩選)
    #達標   -> 停止生成，就取當前round數的前 1 round所篩選的生成資料跟著訓練 (在原本CTC中，是因為沒有加入指定效能，因此，必須根據下一round來判斷是否停止，停止則取前 2 round的生成資料)
#當全體達標，就可以跳出迴圈，並存取模型，傳送weight檔
#(可以將未達別的類別，紀錄有多少個，當達標後，就 -1)
#達標後，將該類別的生成資料存成，例如: temp_data_final.csv
#將所有達標類別的生成資料都放入該資料中，並且從未達標的csv檔中刪除
#最後當裡面沒有任何未達標的csv檔，就會將temp_data_final.csv與原生資料混和後，再訓練模型，並保存該校能
#當到最後要保存效能時，需要不停重複訓練，直至所有類別都是那個效能以上
#可在最後一個類別達標後，就去檢查其餘的都還有沒有到達，沒有的話，就開始上述兩個步驟
#只要生成過，就會保存該類別的 G，以便下次生成用到


